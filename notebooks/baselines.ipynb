{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c84a8ef",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ed2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def deduplicate_entities(entities):\n",
    "    deduplicated = entities\n",
    "    \n",
    "    deduplicated = list(map(lambda x: re.sub(r'in\\'$', 'ing', x).strip(), deduplicated))\n",
    "    deduplicated = list(map(lambda x: re.sub(r'ning$', '', x).strip(), deduplicated))\n",
    "    deduplicated = list(map(lambda x: re.sub(r'ing$', '', x).strip(), deduplicated))\n",
    "    deduplicated = list(map(lambda x: re.sub(r'ed$', '', x).strip(), deduplicated))\n",
    "    \n",
    "    deduplicated = list(set(map(str.lower, deduplicated)))\n",
    "    \n",
    "    return deduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69824e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.lower()\n",
    "    punctuation_ = punctuation.replace(\"'\", \"\").replace(\"-\", \"\")\n",
    "    text = re.sub(f\"[{re.escape(punctuation_)}]+\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"in\\'\\b\", \"ing\", text)\n",
    "    text = re.sub(r\"ning\\b\", \"\", text)\n",
    "    text = re.sub(r\"ing\\b\", \"\", text)\n",
    "    text = re.sub(r\"ed\\b\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43dc94c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = \"../data/labels_manual\"\n",
    "lyrics_path = \"../data/lyrics/\"\n",
    "lyrics_dict = dict()\n",
    "\n",
    "for filename in os.listdir(base_path):\n",
    "    if not filename.endswith(\".json\") or (\"template\" in filename): continue\n",
    "    \n",
    "    file_path = Path(lyrics_path) / filename.replace(\".json\", \".txt\")\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lyrics = f.read().lower()\n",
    "    lyrics = preprocess_text(lyrics)\n",
    "        \n",
    "    file_path = Path(base_path) / filename\n",
    "    with open(file_path, \"r\") as f:\n",
    "        labels = json.load(f)\n",
    "        \n",
    "    for entity_type in labels:\n",
    "        labels[entity_type] = deduplicate_entities(labels[entity_type])\n",
    "        \n",
    "    lyrics_dict[filename.replace(\".json\", \"\")] = {\n",
    "        \"lyrics\": lyrics,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "    \n",
    "len(lyrics_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e4e776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lyrics_dict = dict()\n",
    "\n",
    "with open(\"../data/extraction/MANUAL/test.json\", \"r\") as f:\n",
    "    test = json.load(f)\n",
    "    \n",
    "for values in test:\n",
    "    for entity_type in values[\"entities\"]:\n",
    "        values[\"entities\"][entity_type] = deduplicate_entities(values[\"entities\"][entity_type])\n",
    "        \n",
    "    test_lyrics_dict[values[\"id\"]] = {\n",
    "        \"lyrics\": preprocess_text(values[\"context\"]),\n",
    "        \"labels\": values[\"entities\"]\n",
    "    }\n",
    "    \n",
    "len(test_lyrics_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce33dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Vocab\n",
      "human-powered 6\n",
      "animal-powered 5\n",
      "railways 6\n",
      "roadways 7\n",
      "water_transport 6\n",
      "air_transport 6\n"
     ]
    }
   ],
   "source": [
    "manual_vocab = {\n",
    "    \"human-powered\": [\"bicycle\", \"bike\", \"scooter\", \"skateboard\", \"walking\", \"on foot\"],\n",
    "    \"animal-powered\": [\"horse\", \"carriage\", \"camel\", \"donkey\", \"sleigh\"],\n",
    "    \"railways\": [\"train\", \"subway\", \"metro\", \"tram\", \"railway\", \"railroad\"],\n",
    "    \"roadways\": [\"car\", \"bus\", \"taxi\", \"truck\", \"lorry\", \"jeep\", \"motorcycle\"],\n",
    "    \"water_transport\": [\"boat\", \"ship\", \"ferry\", \"yacht\", \"sailboat\", \"submarine\"],\n",
    "    \"air_transport\": [\"plane\", \"airplane\", \"jet\", \"helicopter\", \"rocket\", \"zeppelin\"],\n",
    "}\n",
    "\n",
    "print(\"Manual Vocab\")\n",
    "for entity_type in manual_vocab:\n",
    "    manual_vocab[entity_type] = deduplicate_entities(manual_vocab[entity_type])\n",
    "    \n",
    "    print(entity_type, len(manual_vocab[entity_type]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3087ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def evaluate_extraction(true_entities_dict, predicted_entities_dict):\n",
    "    true_entities_list = true_entities_dict.values()\n",
    "    predicted_entities_list = predicted_entities_dict.values()\n",
    "    \n",
    "    true_entities_keys = list(true_entities_dict.keys())\n",
    "    predicted_entities_keys = list(predicted_entities_dict.keys())\n",
    "    \n",
    "    assert len(true_entities_list) == len(predicted_entities_list), \"The number of examples must match.\"\n",
    "\n",
    "    # Collect all entity types appearing in any example\n",
    "    entity_types = set()\n",
    "    for true_dict in true_entities_list:\n",
    "        entity_types.update(true_dict.keys())\n",
    "    for pred_dict in predicted_entities_list:\n",
    "        entity_types.update(pred_dict.keys())\n",
    "\n",
    "    # Dictionary to store aggregated counts per entity type\n",
    "    per_entity_counts = {etype: {\"TP\": 0, \"FP\": 0, \"FN\": 0} for etype in entity_types}\n",
    "    fp = defaultdict(list)\n",
    "    fn = defaultdict(list)\n",
    "\n",
    "    # Iterate over all examples and aggregate counts for each entity type\n",
    "    for true_dict, pred_dict, true_key, pred_key in zip(true_entities_list, predicted_entities_list, true_entities_keys, predicted_entities_keys):\n",
    "        for etype in entity_types:\n",
    "            # Get the list of entities for this type; default to empty list if missing.\n",
    "            true_list = true_dict.get(etype, [])\n",
    "            pred_list = pred_dict.get(etype, [])\n",
    "            tmp = []\n",
    "            for entity in pred_list:\n",
    "                if isinstance(entity, str):\n",
    "                    tmp.append(entity)\n",
    "                elif \"text\" in entity:\n",
    "                    tmp.append(entity[\"text\"])\n",
    "                elif \"name\" in entity:\n",
    "                    tmp.append(entity[\"name\"])\n",
    "            pred_list = tmp\n",
    "            pred_list = list(set(pred_list))\n",
    "            \n",
    "            # Use Counter to account for duplicates\n",
    "            true_counter = Counter(true_list)\n",
    "            pred_counter = Counter(pred_list)\n",
    "            \n",
    "            # Count true positives: for each entity present in both, add the minimum count\n",
    "            common_entities = set(true_counter.keys()) & set(pred_counter.keys())\n",
    "            TP = sum(min(true_counter[ent], pred_counter[ent]) for ent in common_entities)\n",
    "            \n",
    "            # Count false positives: predicted count minus the matched count for every predicted entity\n",
    "            FP = sum(pred_counter[ent] - min(true_counter.get(ent, 0), pred_counter[ent]) for ent in pred_counter)\n",
    "            \n",
    "            # Count false negatives: true count minus the matched count for every true entity\n",
    "            FN = sum(true_counter[ent] - min(true_counter[ent], pred_counter.get(ent, 0)) for ent in true_counter)\n",
    "            \n",
    "            # Aggregate counts\n",
    "            per_entity_counts[etype][\"TP\"] += TP\n",
    "            per_entity_counts[etype][\"FP\"] += FP\n",
    "            per_entity_counts[etype][\"FN\"] += FN\n",
    "            \n",
    "            if FP > 0:\n",
    "                fp[true_key].append({\"entity\": etype, \"true\": true_list, \"pred\": pred_list})\n",
    "            if FN > 0:\n",
    "                fn[true_key].append({\"entity\": etype, \"true\": true_list, \"pred\": pred_list})\n",
    "\n",
    "    # Compute precision, recall, and F1 for each entity type\n",
    "    per_entity_results = {}\n",
    "    for etype, counts in per_entity_counts.items():\n",
    "        TP = counts[\"TP\"]\n",
    "        FP = counts[\"FP\"]\n",
    "        FN = counts[\"FN\"]\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "        recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        f1        = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        per_entity_results[etype] = {\"Precision\": round(precision, 3), \"Recall\": round(recall, 3), \"F1\": round(f1, 3)}\n",
    "\n",
    "    # Macro-average: average the metric scores over all entity types\n",
    "    macro_precision = sum(result[\"Precision\"] for result in per_entity_results.values()) / len(entity_types)\n",
    "    macro_recall    = sum(result[\"Recall\"] for result in per_entity_results.values()) / len(entity_types)\n",
    "    macro_f1        = sum(result[\"F1\"] for result in per_entity_results.values()) / len(entity_types)\n",
    "    \n",
    "    # Micro-average: aggregate counts over all entity types\n",
    "    total_TP = sum(counts[\"TP\"] for counts in per_entity_counts.values())\n",
    "    total_FP = sum(counts[\"FP\"] for counts in per_entity_counts.values())\n",
    "    total_FN = sum(counts[\"FN\"] for counts in per_entity_counts.values())\n",
    "\n",
    "    micro_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0.0\n",
    "    micro_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0.0\n",
    "    micro_f1 = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0\n",
    "    \n",
    "    results = {\n",
    "        \"Macro Precision\": macro_precision,\n",
    "        \"Macro Recall\": macro_recall,\n",
    "        \"Macro F1\": macro_f1,\n",
    "        \"Micro Precision\": micro_precision,\n",
    "        \"Micro Recall\": micro_recall,\n",
    "        \"Micro F1\": micro_f1\n",
    "    }\n",
    "    results = {metric: [round(value, 3)] for metric, value in results.items()}\n",
    "    \n",
    "    return per_entity_results, results, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825cefa7",
   "metadata": {},
   "source": [
    "# Baseline 1: Rule-based keywords matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90f75bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "vocab_ = manual_vocab\n",
    "\n",
    "def predict(text):\n",
    "    predictions = dict()\n",
    "    \n",
    "    for entity_type in vocab_:\n",
    "        predictions[entity_type] = []\n",
    "        \n",
    "        for entity in vocab_[entity_type]:\n",
    "            entity = entity.lower()\n",
    "            pattern = r'\\b' + re.escape(entity) + r's?\\b'\n",
    "            if re.search(pattern, text):\n",
    "                predictions[entity_type].append(entity)\n",
    "                \n",
    "        predictions[entity_type] = deduplicate_entities(predictions[entity_type])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c8110c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435120adf2174bd3a5d037df25078c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for id_, item in tqdm(test_lyrics_dict.items()):\n",
    "    test_lyrics_dict[id_][\"rule-based\"] = predict(item[\"lyrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f59f11b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'water_transport': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'human-powered': {'Precision': 0.25, 'Recall': 0.2, 'F1': 0.222},\n",
       " 'air_transport': {'Precision': 0.333, 'Recall': 0.25, 'F1': 0.286},\n",
       " 'railways': {'Precision': 0.333, 'Recall': 1.0, 'F1': 0.5},\n",
       " 'animal-powered': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'roadways': {'Precision': 0.375, 'Recall': 0.15, 'F1': 0.214}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = {id_: item[\"labels\"] for id_, item in test_lyrics_dict.items()}\n",
    "predicted_labels = {id_: item[\"rule-based\"] for id_, item in test_lyrics_dict.items()}\n",
    "\n",
    "per_entity_results, overall_results, false_positives, false_negatives = evaluate_extraction(true_labels, predicted_labels)\n",
    "per_entity_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7fc82",
   "metadata": {},
   "source": [
    "# Baseline 2: Clustering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e07ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "path = '../data/w2v/GoogleNews-vectors-negative300.bin.gz'\n",
    "model = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60f61ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6c089eb1b0418ab9b2661058418f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_noun_verb_tokens(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return [token.text for token in doc if token.pos_ in (\"NOUN\", \"VERB\") and token.text not in stop_words]\n",
    "\n",
    "lyrics_list = [lyrics_dict[id_][\"lyrics\"] for id_ in lyrics_dict]\n",
    "\n",
    "sentences = []\n",
    "for lyric in tqdm(lyrics_list):\n",
    "    tokens = extract_noun_verb_tokens(lyric)\n",
    "    sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31603f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=300,\n",
    "    window=15,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "483793aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ = model # w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "779096e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed_mention(text):\n",
    "    tokens = [t for t in text.lower().split() if t in w2v_]\n",
    "    if not tokens:\n",
    "        return np.zeros(w2v_.vector_size)\n",
    "    return np.mean([w2v_[t] for t in tokens], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5063fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine(a, b):\n",
    "    d = norm(a)*norm(b)\n",
    "    return float(np.dot(a, b) / d) if d != 0 else 0.0\n",
    "\n",
    "def build_class_prototypes(w2v, class_seeds):\n",
    "    prototypes = {}\n",
    "    for cls, seeds in class_seeds.items():\n",
    "        vecs = [embed_mention(w) for w in seeds if w in w2v]\n",
    "        if vecs:\n",
    "            prototypes[cls] = np.mean(vecs, axis=0)\n",
    "    return prototypes\n",
    "\n",
    "class_vecs = build_class_prototypes(w2v_, manual_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "96911777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_transport_sim(word, w2v, class_vecs):\n",
    "    if word not in w2v:\n",
    "        return 0.0, None\n",
    "    v = w2v[word]\n",
    "    best_cls, best_sim = None, -1\n",
    "    for cls, pvec in class_vecs.items():\n",
    "        sim = cosine(v, pvec)\n",
    "        if sim > best_sim:\n",
    "            best_cls, best_sim = cls, sim\n",
    "    return best_sim, best_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7d9aec45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b300e01d988f45e599962c346e7db125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3515, 300)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "words = list(w2v_.key_to_index.keys()) # all words in vocab\n",
    "\n",
    "candidate_words = []\n",
    "for w in tqdm(words):\n",
    "    if w not in w2v_:\n",
    "        continue\n",
    "    sim, _ = max_transport_sim(w, w2v_, class_vecs)\n",
    "    if sim >= 0.5:\n",
    "        candidate_words.append(w)\n",
    "\n",
    "embeddings = np.vstack([embed_mention(w) for w in candidate_words])\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c64dafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K = 50\n",
    "kmeans = KMeans(n_clusters=K, n_init=10, random_state=0)\n",
    "cluster_ids = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# mapping: word -> cluster id\n",
    "word_to_cluster = {w: cid for w, cid in zip(candidate_words, cluster_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d4f056fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'water_transport': 17,\n",
       "         'air_transport': 10,\n",
       "         'roadways': 9,\n",
       "         'railways': 8,\n",
       "         'human-powered': 3,\n",
       "         'OTHER': 2,\n",
       "         'animal-powered': 1})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def score_cluster(cid, candidate_words, word_to_cluster, w2v, class_vecs):\n",
    "    words_in_cluster = [w for w in candidate_words if word_to_cluster[w] == cid]\n",
    "    scores = defaultdict(float)\n",
    "    count = 0\n",
    "\n",
    "    for w in words_in_cluster:\n",
    "        if w not in w2v:\n",
    "            continue\n",
    "        v = w2v[w]\n",
    "        for cls, pvec in class_vecs.items():\n",
    "            scores[cls] += cosine(v, pvec)\n",
    "        count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return None\n",
    "\n",
    "    # normalize by cluster size to avoid bias toward large clusters\n",
    "    for cls in scores:\n",
    "        scores[cls] /= count\n",
    "    return dict(scores)\n",
    "\n",
    "def assign_cluster_types(K, candidate_words, word_to_cluster, w2v, class_vecs, threshold=0.5):\n",
    "    cluster_to_type = {}\n",
    "    for cid in range(K):\n",
    "        scores = score_cluster(cid, candidate_words, word_to_cluster, w2v, class_vecs)\n",
    "        if not scores:\n",
    "            cluster_to_type[cid] = \"OTHER\"\n",
    "            continue\n",
    "        best_cls, best_score = max(scores.items(), key=lambda kv: kv[1])\n",
    "        cluster_to_type[cid] = best_cls if best_score >= threshold else \"OTHER\"\n",
    "    return cluster_to_type\n",
    "\n",
    "cluster_to_type = assign_cluster_types(K, candidate_words, word_to_cluster, w2v_, class_vecs)\n",
    "\n",
    "Counter(cluster_to_type.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dccea190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_token(tok, w2v, word_to_cluster, cluster_to_type, class_vecs, sim_threshold=0.35):\n",
    "    t = tok.lower()\n",
    "    if t in word_to_cluster:\n",
    "        cls = cluster_to_type[word_to_cluster[t]]\n",
    "        if cls == \"OTHER\":\n",
    "            return \"OTHER\"\n",
    "        sim, best_cls = max_transport_sim(t, w2v, class_vecs)\n",
    "        return cls if (sim >= sim_threshold) and (best_cls == cls) else \"OTHER\"\n",
    "    elif t in w2v:\n",
    "        sim, best_cls = max_transport_sim(t, w2v, class_vecs)\n",
    "        return best_cls if sim >= sim_threshold else \"OTHER\"\n",
    "    else:\n",
    "        return \"OTHER\"\n",
    "    \n",
    "# label_token(\"car\", w2v_, word_to_cluster, cluster_to_type, class_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8c5dd37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f808c3c12849508d4fcba5b1142737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for id_, item in tqdm(test_lyrics_dict.items()):\n",
    "    tokens = extract_noun_verb_tokens(item[\"lyrics\"])\n",
    "    predictions = dict()\n",
    "    for entity_type in manual_vocab:\n",
    "        predictions[entity_type] = []\n",
    "    for tok in tokens:\n",
    "        label = label_token(tok, w2v_, word_to_cluster, cluster_to_type, class_vecs, sim_threshold=0.7)\n",
    "        if label != \"OTHER\":\n",
    "            predictions[label].append(tok)\n",
    "    for entity_type in predictions:\n",
    "        predictions[entity_type] = deduplicate_entities(predictions[entity_type])\n",
    "    test_lyrics_dict[id_][\"clustering\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "20d46b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'water_transport': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'human-powered': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'air_transport': {'Precision': 0.333, 'Recall': 0.25, 'F1': 0.286},\n",
       " 'railways': {'Precision': 0.5, 'Recall': 1.0, 'F1': 0.667},\n",
       " 'animal-powered': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'roadways': {'Precision': 0.429, 'Recall': 0.15, 'F1': 0.222}}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = {id_: item[\"labels\"] for id_, item in test_lyrics_dict.items()}\n",
    "predicted_labels = {id_: item[\"clustering\"] for id_, item in test_lyrics_dict.items()}\n",
    "\n",
    "per_entity_results, overall_results, false_positives, false_negatives = evaluate_extraction(true_labels, predicted_labels)\n",
    "per_entity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbac30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "31033b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'water_transport': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'human-powered': {'Precision': 0.002, 'Recall': 0.2, 'F1': 0.003},\n",
       " 'air_transport': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'railways': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'animal-powered': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'roadways': {'Precision': 0.003, 'Recall': 0.2, 'F1': 0.006}}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = {id_: item[\"labels\"] for id_, item in test_lyrics_dict.items()}\n",
    "predicted_labels = {id_: item[\"clustering\"] for id_, item in test_lyrics_dict.items()}\n",
    "\n",
    "per_entity_results, overall_results, false_positives, false_negatives = evaluate_extraction(true_labels, predicted_labels)\n",
    "per_entity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad2472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a82c42d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'water_transport': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'human-powered': {'Precision': 0.1, 'Recall': 0.2, 'F1': 0.133},\n",
       " 'air_transport': {'Precision': 0.048, 'Recall': 0.25, 'F1': 0.08},\n",
       " 'railways': {'Precision': 0.033, 'Recall': 1.0, 'F1': 0.065},\n",
       " 'animal-powered': {'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0},\n",
       " 'roadways': {'Precision': 0.318, 'Recall': 0.35, 'F1': 0.333}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = {id_: item[\"labels\"] for id_, item in test_lyrics_dict.items()}\n",
    "predicted_labels = {id_: item[\"clustering\"] for id_, item in test_lyrics_dict.items()}\n",
    "\n",
    "per_entity_results, overall_results, false_positives, false_negatives = evaluate_extraction(true_labels, predicted_labels)\n",
    "per_entity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a538fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "44e85d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['stop', 'lights', \"nothin'\", 'drop', 'lil', 'hurt', 'swear', 'lookin', 'sky', 'hoes']\n",
      "\n",
      "Cluster 1\n",
      "['water', 'lady', 'peat', 'gett', 'birthday', 'feelin', 'wonder', 'hang', 'dog', 'jeans']\n",
      "\n",
      "Cluster 2\n",
      "['na']\n",
      "\n",
      "Cluster 3\n",
      "['ways', 'shoes', 'fit', 'fell', 'shorty', 'lay', 'family', 'space', 'fool', 'finish']\n",
      "\n",
      "Cluster 4\n",
      "['bitch', 'money', 'r']\n",
      "\n",
      "Cluster 5\n",
      "['hope', 'lost', 'gets', 'fight', 'dress', 'house', 'honey', 'livin', 'power', 'cash']\n",
      "\n",
      "Cluster 6\n",
      "['doo']\n",
      "\n",
      "Cluster 7\n",
      "['got', 'aingt', 'think', 'done', 'wait', 'boy', 'hands', 'ooh']\n",
      "\n",
      "Cluster 8\n",
      "['tonight', 'babe']\n",
      "\n",
      "Cluster 9\n",
      "['choose', 'celebrate', 'known', 'climb', 'daylight', 'luck', 'low', 'logic', 'weather', 'thicke']\n",
      "\n",
      "Cluster 10\n",
      "['thunder']\n",
      "\n",
      "Cluster 11\n",
      "['bam']\n",
      "\n",
      "Cluster 12\n",
      "['wick']\n",
      "\n",
      "Cluster 13\n",
      "['fuck', 'shit', 'put', 'man', 'hit', 'niggas', 'nigga', 'dance', 'ass', 'lot']\n",
      "\n",
      "Cluster 14\n",
      "['get', 'go', 'say', 'make', '-', 'come', 'give', 'talk', 'stay', 'day']\n",
      "\n",
      "Cluster 15\n",
      "['turn', 'pull', 'br', 'ones']\n",
      "\n",
      "Cluster 16\n",
      "['comin']\n",
      "\n",
      "Cluster 17\n",
      "['love']\n",
      "\n",
      "Cluster 18\n",
      "['intro', 'scream', 'tak', 'kick', 'learn', 'gaga', 'found', 'words', 'step', 'thank']\n",
      "\n",
      "Cluster 19\n",
      "['arm', 'history', 'faces', 'actin', 'mornin', 'steal', 'boogie', 'brother', 'wave', 'bags']\n",
      "\n",
      "Cluster 20\n",
      "['taste']\n",
      "\n",
      "Cluster 21\n",
      "['home', 'set', 'ayy', 'city', 'soul', 'holdin', 'sun', 'moment', 'boyfriend', 'girlfriend']\n",
      "\n",
      "Cluster 22\n",
      "['said', 'mind', 'call', 'like', 'head', 'break', 'girls', 'blow', 'rollie']\n",
      "\n",
      "Cluster 23\n",
      "['skin', 'moves', 'woah', 'adore', 'pocket', 'bottles', 'shut', 'reason', 'spent', 'motion']\n",
      "\n",
      "Cluster 24\n",
      "['sean', 'folks', 'grab', 'places', 'ocean', 'bein', 'pressure', 'showin', 'chill', 'guys']\n",
      "\n",
      "Cluster 25\n",
      "['look', 'made']\n",
      "\n",
      "Cluster 26\n",
      "['know', 'tell', 'one', 'find', 'fall']\n",
      "\n",
      "Cluster 27\n",
      "['hate', 'dream', 'ride', 'mine', 'type', 'ask', 'sleep', 'fuckin', 'pretend', 'wake']\n",
      "\n",
      "Cluster 28\n",
      "['time', 'eyes', 'gone', 'play', 'friends', 'bottoms', 'bitches', 'came', 'club', 'cup']\n",
      "\n",
      "Cluster 29\n",
      "['keep', 'tryna', 'left', 'hear', \"goin'\", 'burn', 'oh', 'pain', 'help', 'hours']\n",
      "\n",
      "Cluster 30\n",
      "['verse', 'woo', 'everyth', 'things', 'knew', 'name', 'beat', 'party', 'took', 'side']\n",
      "\n",
      "Cluster 31\n",
      "['light']\n",
      "\n",
      "Cluster 32\n",
      "['wanna', 'show']\n",
      "\n",
      "Cluster 33\n",
      "['let', 'want', 'girl']\n",
      "\n",
      "Cluster 34\n",
      "['wish']\n",
      "\n",
      "Cluster 35\n",
      "['way', 'born']\n",
      "\n",
      "Cluster 36\n",
      "['baby']\n",
      "\n",
      "Cluster 37\n",
      "['tri', 'lift', 'ahh', 'boss', 'paid', 'cake', 'clos', 'problems', 'spark', 'peace']\n",
      "\n",
      "Cluster 38\n",
      "['feel']\n",
      "\n",
      "Cluster 39\n",
      "['bass']\n",
      "\n",
      "Cluster 40\n",
      "['daddy', 'memories']\n",
      "\n",
      "Cluster 41\n",
      "['start', 'thought', 'face', 'believe', 'move', 'back', 'lov', 'die', 'care', 'feels']\n",
      "\n",
      "Cluster 42\n",
      "['life', 'night', 'world', 'run', 'fire', 'b', 'stars']\n",
      "\n",
      "Cluster 43\n",
      "['hold']\n",
      "\n",
      "Cluster 44\n",
      "['body']\n",
      "\n",
      "Cluster 45\n",
      "['teh', 'shark']\n",
      "\n",
      "Cluster 46\n",
      "['talkin', 'tryin', 'seen', 'diamonds', 'control', 'quan', 'scar', 'gang', 'e', 'moon']\n",
      "\n",
      "Cluster 47\n",
      "['chorus', 'heart', 'bridge', 'wants']\n",
      "\n",
      "Cluster 48\n",
      "['gon', 'see', 'take', 'ne', 'pre', 'try', 'noth', 'post', 'touch', 'rock']\n",
      "\n",
      "Cluster 49\n",
      "['shots']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freq = Counter(w for sent in sentences for w in sent)\n",
    "\n",
    "clusters = {cid: [] for cid in range(K)}\n",
    "for w, cid in word_to_cluster.items():\n",
    "    clusters[cid].append(w)\n",
    "\n",
    "for cid in range(K):\n",
    "    print(f\"\\nCluster {cid}\")\n",
    "    print(sorted(clusters[cid], key=lambda w: -freq[w])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fdb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
